{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2757da69",
   "metadata": {},
   "source": [
    "## 11장 텍스트를 위한 딥러닝\n",
    "\n",
    "11.1 자연어 처리 소개 <br>\n",
    "11.2 텍스트 데이터 준비 <br>\n",
    "11.3 단어 그룹을 표현하는 두 가지 방법: 집합과 시퀀스 <br>\n",
    "11.4 텍스트 분류를 넘어: seq-to-seq 학습 <br>\n",
    "11.5 요약 <br>\n",
    "\n",
    "### 11.1 자연어 처리 소개\n",
    "사람의 언어를 assembly, LISP, XML같은 기계를 위해 고안된 언어와 구별하기 위해 그렇게 부름 <br>\n",
    "사람의 언어는 먼저 사용되고 나중에 규칙이 생김, 고정된 어휘에서 정확하게 정의 된 개념을 표현하기 위해 정확한 문법 규칙을 사용함 <br>\n",
    "<b> 언어는 거의 모든 지식을 저장하는 방법, 사람의 생각 자체가 대부분 언어에 기반을 둔다</b> <br>\n",
    "응용 언어학의 입장에서 시도되었음: <br>\n",
    "- 1960s ~ 1990s:  ELIZA 프로그램이 패턴 매칭을 사용해서 매우 기초적인 대화를 수행 <br>\n",
    "- 1980s 후반: 자연어 처리에 머신러닝 방법을 적용, 결정트리 기반-->if/then/else같은 규칙을 자동으로 개발하려는 의도, logistic 회귀와 같은 통계적 방법이 퍼짐 <br>\n",
    "- 1990s: Frederick Jelink은 \"언어학자를 해고할떄마다 음성 인식기 성능이 올라간다\" <br><br>\n",
    "\n",
    "\n",
    "현대적인 NLP는 입력으로 언어를 받아 유용한 어떤 것을 반환하는 것: <br>\n",
    "- \"이 글의 주제는 무엇인가요?\" (텍스트 분류) <br>\n",
    "- \"이 텍스트에 부적절한 내용이 들어있나요?\" (콘텐츠 필터링) <br>\n",
    "- \"이 텍스트가 긍정적인가요? 아니면 부정적인가요?\" (감성 분석) <br>\n",
    "- \"이 문장은 독일어로 어떻게 되나요?\" (번역) <br>\n",
    "- \"이 글을 하나의 문단으로 요약하면 어떻게 되나요?\" (요약) <br>\n",
    "- 기타 <br>\n",
    "\n",
    "모델은 입력에 있는 통계적인 규칙성을 찾는 것뿐이며 이는 여러가지 간단한 작업을 잘 수행하는데 충분합니다. <br>\n",
    "LSTM은 1990s 후반에 나온 시퀀스 처리 알고리즘, 2015년에 Keras가 오픈소스로 제공 <br>\n",
    "2015 ~ 2017 까지 NLP의 급성장, 특히 양방향 LSTM모델이 요약(summarization)에서 질문-대답(question-answering), 기계 번역까지..! <br>\n",
    "2017 ~ 2018 즈음에 Transformer <br><br>\n",
    "\n",
    "\n",
    "### 11.2 텍스트 데이터 준비 \n",
    "<b>Text Vectorization (텍스트 벡터화)</b>는 텍스트를 수치 텐서로 바꾸는 과정, 하는 이유는 미분 가능한 함수인 딥러닝 모델은 수치 텐서만 처리할 수 있다, 원시 텍스트를 입력으로 사용할 수 없다. 다음은 텍스트 벡터화 과정임\n",
    "1. <b>표준화(standardization)</b>한다. 소문자로 바꾸거나 구두점을 제거하는 등의 작업 <br>\n",
    "2. 텍스트를 <b>토큰(Token)</b>단위로 분할한다. 예를 들어 문자, 단어, 단어의 그룹, 이를 <b>Tokenization(토큰화)</b>라고 부른다.<br>\n",
    "3. 각 토큰을 수치 벡터로 바꾼다. 일반적으로 먼저 데이터에 등장하는 모든 토큰을 <b>인덱싱(Indexing)</b>한다.<br><br>\n",
    "\n",
    "\n",
    "#### 11.2.1 텍스트 표준화\n",
    "\"sunset came. i was staring at the Mexico sky. isnt nature splendid??\" <br>\n",
    "\"Sunset came; I stared at the Mexico sky. isn't nature splendid?\" <br>\n",
    "텍스트 표준화는 모델이 인코딩 차이를 고려하지 않도록 이를 제거하기 위한 기초적인 특성 공학의 한 형태, 표준화 방법 중 하나는 '소문자로 바꾸고 구두점 문자를 삭제'하는 것 <br>\n",
    "이렇게 변한다. <br>\n",
    "\"sunset came i was staring at the mexico sky isnt nature splendid\" <br>\n",
    "\"sunset came i stared at the mexico sky isnt nature splendid\" <br><br>\n",
    "\n",
    "또 다른 방법) 머신러닝에서 드물게 사용되는 고급 표준화 패턴 <b>어간 추출(stemming)</b>: <br>\n",
    "(동사의 여러 활용 형태처럼) 어형이 변형된 단어를 공통된 하나의 표현으로 바꾸는 것<br>\n",
    "ex) \"caught\"와 \"been catching\"을 \"[catch]\"로 바꾸거나 \"cats\"를 \"cat\"으로 바꾸는 식 <br>\n",
    "그렇게 되면 비슷한 두 문장이 결국 동일한 인코딩을 가지게 된다. <br><br>\n",
    "\n",
    "\n",
    "#### 11.2.2 텍스트 분할(토큰화)\n",
    "텍스트를 표준화하고 나면 벡터화할 단위(토큰)으로 나누어야 한다. 이 단계를 토크화(Tokenization)이라고 부름. 3가지 방법으로 수행<br>\n",
    "- <b>단어 수준 토큰화</b>: 토큰이 공백으로 (또는 구두점으로) 구분된 부분 문자열 <br>\n",
    "- <b>N-그램(n-gram) 토큰화</b>: 토큰이 N개의 연속된 단어 그룹이다 <br>\n",
    "- <b>문자 수준 토큰화</b>: 각 문자가 하나의 토큰이다. <br>\n",
    "\n",
    "일반적으로 단어 수준 토큰화나 N-그램 토큰화를 항상 사용할것임. 단어싀 순서를 고려하는 시퀀스 모델(sequence model)과 입력 단어의 순서를 무시하고 집합으로 다루는 BoW모델(Bag-of-Words model)이다. \n",
    "\n",
    "__cf) N-그램과 BoW__\n",
    "단어 N-그램은 문장에서 추출한 N개 (또는 그 이하)의 연속된 단어 그룹인데, 2-그램의 집합 또는 3-그램의 집합으로 분해할 수 있음. <br>\n",
    "{the , the cat, cat, cat sat, sat, <br>\n",
    "sat on, on, on the, the mat, mat} <br><br>\n",
    "\n",
    "{the , the cat, cat, cat sat, the cat sat, <br>\n",
    "sat, sat on, on, cat sat on, on the, <br>\n",
    "sat on the, the mat, mat, on the mat} <br><br>\n",
    "\n",
    "이런 집합을 각각 2-그램 가방(bag of 2-gram) 또는 3-그램 가방(bag of 3-gram)이라고 한다. 가방이란 용어는 다루고자 하는 것이 리스트나 시퀀스가 아니라 토큰의 집합이라는 사시을 의미한다. 이 토큰에는 특정한 순서가 없다. <br><br>\n",
    "\n",
    "#### 11.2.3 어휘 사전 인덱싱\n",
    "텍스트를 토큰으로 나눈 후 각 토큰을 수치 표현으로 인코딩해야 한다. 토큰을 해싱(hasing)하여 고정 크기의 이진 벡터로 바꾸는 것처럼 상태가 없는 방식을 사용할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ce66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {}\n",
    "for text in dataset:\n",
    "    text = standardize(text)\n",
    "    tokens = tokenize(text)\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560fe906",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc1b217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f6ab28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4a0858",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9256e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960c24db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668f37d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d718f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b1db8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810cf2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289281c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebe13da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4b19c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469e1b69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
