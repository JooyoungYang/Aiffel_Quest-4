{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9baf608",
   "metadata": {},
   "source": [
    "## 4 신경망 시작하기: 분류와 회귀 \n",
    "\n",
    "1 영화리뷰분류: 이진 분류 문제 <br>\n",
    "2 뉴스 기사 분류: 다중 분류 문제 <br>\n",
    "3 주택 가격 예측: 회귀 문제 <br>\n",
    "4 요약 <br>\n",
    "\n",
    "\n",
    "### 분류와 회귀에서 사용하는 용어 \n",
    "\n",
    "- <b>샘플</b> 또는 <b>입력</b>: 모델에 주입될 하나의 데이터 포인트(data point) <br>\n",
    "- <b>예측</b> 또는 <b>출력</b>: 모델로부터 나오는 값 <br>\n",
    "- <b>타깃</b>: 정답. 외부 데이터 소스에 근거하여 모델이 완벽하게 예측해야 하는 값 <br>\n",
    "- <b>예측 오차</b> 또는 <b> 손실 값</b>: 모델의 예측과 타깃 사이의 거리를 측정한 값 <br>\n",
    "- <b>클래스</b>: 분류 문제에서 선택할 수 있는 가능한 레이블의 집합. ex, 모델에 주입될 하나의 데이터 포인트(data point) <br>\n",
    "- <b>레이블</b>: 분류 문제에서 클래스 할당의 구체적인 사례.ex, #1234에 '강아지' 클래스가 들어 있다고 표시한다면 '강아지'는 사진 #1234의 레이블이 된다 <br>\n",
    "- <b>참값(ground-truth)</b> 또는 <b>애너테이션(annotation)</b>: 데이터셋이 대한 모든 타깃. 일반적으로 사람에 의해 수집됨 <br>\n",
    "- <b>이진 분류</b>: 각 입력 샘플이 2개의 배타적인 범주로 구분되는 분류 작업 <br>\n",
    "- <b>다중 분류</b>: 각 입력 샘플이 2개 이상의 범주로 구분되는 분류 작업. ex, 손글씨 숫자 분류를 말함 <br>\n",
    "- <b>다중 레이블 분류</b>: 각 입력 샘플이 여러개의 레이블에 할당될 수 있는 분류 작업. ex, 하나의 이미지에 고양이와 강아지가 모두 들어 있을 떄는 '고양이' 레이블과 '강아지' 레이블을 모두 할당해야 한다. 보통 이미지마다 레이블의 개수는 다름 <br>\n",
    "- <b>스칼라 회귀</b>: 타깃이 연속적인 스칼라 값인 작업. 주택 가격 예측이 좋은 예, 각기 다른 타깃 가격이 연속적인 공간을 형성함 <br>\n",
    "- <b>벡터 회귀</b>: 타깃이 연속적인 값의 집합인 작업. 예를 들어 연속적인 값으로 이루어진 벡터. (이미지에 있는 경계 사자 (bounding box)의 좌표 같은) 여러개의 값에 대한 회귀를 한다면 벡터 회귀 <br>\n",
    "- <b>미니 배치</b> 또는 <b>배치</b>: 모델에 의해 동시에 처리되는 소량의 샘플 묶음(일반적으로 8개 에서 128개 사이). 샘플 개수는 GPU 메모리의 할당이 용이하도록 2의 거듭제곱으로 하는 경우가 많다. 훈련할 떄 미니 배치마다 한번씩 모델의 가중치에 적용할 경사 하강법 업데이트 값을 계산 <br><br>\n",
    "\n",
    "\n",
    "### 4.1 영화 리뷰 분류: 이진 분류 문제\n",
    "two-class classification or binary classification이라고도 부름 <br>\n",
    "\n",
    "#### 4.1.1 IMDB 데이터셋 \n",
    "5만개가 있고 부정, 긍정 각각 50%, 50% <br>\n",
    "\n",
    "#### 코드 4-1 IMDB 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c23743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1970b96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "218\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_data[0])\n",
    "print(len(train_data[0])) # 218\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02d7bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])  # 9999"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231ef326",
   "metadata": {},
   "source": [
    "#### 코드 4-2 리뷰를 다시 텍스트로 디코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6c9c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])  # 정수 인덱스와 단어를 매핑하도록 뒤집는다\n",
    "decoded_review = \" \".join(\n",
    "    [reverse_word_index.get(i-3, \"?\") for i in train_data[0]]) \n",
    "# 리뷰를 디코딩한다. 0,1,2는 '패딩', '문서 시작', '사전에 없음'을 위해 예약되어 있으므로 인덱스에서 3을 뺌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3bf9ad",
   "metadata": {},
   "source": [
    "### 4.1.2 데이터 준비\n",
    "\n",
    "리스트를 텐서로 바꾸는 2가지 방법)\n",
    "- 리스트에 패딩(padding)을 추가하고 (samples, max_length) 크기의 정수 텐서로 변환 <br>\n",
    "- 리스트를 멀티-핫 인코딩(multi-hot encoding)하여 0과 1의 벡터로 변환 <br>\n",
    "- 여기서는 2번째 방법을 적용\n",
    "\n",
    "#### 코드 4-3 정수 시퀀스를 멀티-핫 인코딩으로 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e0e359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def vectorize_sequences(sequences, dimension=100000):\n",
    "    results = np.zeros((len(sequences), dimension))  # 크기가 (len(sequences), dimension)이고 모든 원소가 0인 행렬을 만듦\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        for j in sequence:\n",
    "            results[i,j] = 1  # results[i]에서 특정 인덱스의 위치를 1로 만듦\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)   # 훈련 데이터를 벡터로 변환함\n",
    "x_test = vectorize_sequences(test_data)   # 테스트 데이터를 벡터로 변홤함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7f6217",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ce8f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.asarray(train_labels).astype(\"float32\")\n",
    "y_test = np.asarray(test_labels).astype(\"float32\")\n",
    "\n",
    "# 신경망에 주입할 데이터가 준비되었음!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af0283",
   "metadata": {},
   "source": [
    "### 4.1.3 신경망 모델 만들기\n",
    "\n",
    "- 레이블은 스칼라(1 또는 0)\n",
    "- Dense층을 쌓을 때 2가지 중요한 구조상의 결정이 필요:\n",
    "    - 얼마나 많은 층을 사용할 것인가? <br>\n",
    "    - 각 층에 얼마나 많은 유닛을 둘 것인가? <br>\n",
    "    (책에서는) <br>\n",
    "    - 16개의 유닛을 가진 2개의 증간층 <br>\n",
    "    - 현재 리뷰의 감정을 scalar 값의 예측으루 출력하는 3번째 층 <br>\n",
    "    \n",
    "#### 코드 4-4 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1807e196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'), # 첫번째 매개변수는 층의 unit개수\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e29b75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = relu(dot(W, input)+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674b71bb",
   "metadata": {},
   "source": [
    "#### 활성화 함수? 필요한 이유?\n",
    "\n",
    "- relu와 같은 활성화 함수( or non-linearity)라고도 부름, 이게 없다면 Dense층은 선형적인 연산인 점곱과 덧셈 2개로 구성 <br>\n",
    "output = dot(W, input) + b <br><br>\n",
    "\n",
    "- 선형변환(아핀변환)만 학습 <br>\n",
    "- 이 층의 가설 공간은 입력 데이터를 16차원의 공간으로 바꾸는 가능한 모든 선혀 변환의 집합 <br>\n",
    "- 가설 공간을 풍부하게 만들어 층을 깊게 만드느 장점을 살리기 위해서는 비선형성 or 활성화 함수를 추가해야 함 <br><br>\n",
    "\n",
    "\n",
    "마지막으로 손실 함수와 옵티마이저를 선택 <br>\n",
    "crossentropy는 정보이론(information theory)분야에서 온 개념으로 확률분포간의 차이를 측정 <br>\n",
    "\n",
    "#### 코드 4-5 모델 컴파일하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa48cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1610319",
   "metadata": {},
   "source": [
    "### 4.1.4 훈련검증\n",
    "\n",
    "검증세트를 활용하여 훈련 과정 중에 모델의 정확도를 모니터링하는 것이 표준 관행.\n",
    "\n",
    "#### 코드 4-6 검증 세트 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8199f7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f786ab",
   "metadata": {},
   "source": [
    "#### 코드 4-7 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ab106",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "         partial_y_train, \n",
    "         epochs = 20,\n",
    "         batch_size = 512,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e3f6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_dict= history.history\n",
    "print(history_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8d85da",
   "metadata": {},
   "source": [
    "#### 코드 4-8 훈련과 검증 손실 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf3caf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hitorry_dict = history.history\n",
    "loss_values = history_dict[\"loss\"]\n",
    "val_loss_values = history_dict[\"val_loss\"]\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "plt.plot(epochs, loss_values, \"bo\", label=\"Training loss\")  # 'bo'는 파란색점을 의미\n",
    "plt.plot(epochs, val_loss_values, \"b\", label=\"Validation loss\")  # 'b'는 파란색 실선을 의미\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d8ab8d",
   "metadata": {},
   "source": [
    "#### 코드 4-9 훈련과 검증 정확도 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627d6341",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()  # 그래프를 초기화\n",
    "acc = history_dict[\"accuracy\"]\n",
    "val_acc = history_dict[\"val_accuracy\"]\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training acc\")\n",
    "plt.plot(epochs, val_acc, \"bo\", label=\"Validation acc\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 결과가 훈련 손실이 에포크마다 감소하고 훈련 정확도는 에포크마다 증가\n",
    "# 경사 하강법 최적화를 사용했을 떄 반복마다 최소화된느 것이 손실이므로 기대했던 대로\n",
    "# 과대적합(overfitting) 되었다고 할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdff990",
   "metadata": {},
   "source": [
    "#### 코드 4-10 모델을 처음부터 다시 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0eff7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),\n",
    "    \n",
    "])\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss =\"binary_crossentropy\",\n",
    "             metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, epochs=4, batch_size=512)\n",
    "results = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71182f61",
   "metadata": {},
   "source": [
    "### 4.1.5 훈련될 모델로 새로운 데이터에 대해 예측하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25ffb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbc77e0",
   "metadata": {},
   "source": [
    "### 4.1.6 추가 실험\n",
    "\n",
    "- 여기서는 최종 분류 층 이전에 2개의 표현층을 사용했음, 1개 or 3개의 표현 층을 사용하고 검증과 테스트 정확도에 어떤 영향을 미치는지 확인 <br>\n",
    "- 층의 유닛을 추가하거나 줄여보세요, 32개의 유닛, 64개의 유닛 <br>\n",
    "- binary_crossentropy대신 mse 손실 함수를 사용 <br>\n",
    "- relu 대신 tanh 활성화 함수(초창기 신경망에서 인기 있었던 함수) <br>\n",
    "\n",
    "### 4.1.7 정리\n",
    "\n",
    "- 원본 데이터를 신경망에 텐서로 주입하기 위해서는 꽤 많은 전처리가 필요\n",
    "- relu활성화 함수와 함께 Dense층을 쌓은 모델은 (감성 분류를 포함하여) 여러 종류의 문제에 적용할 수 있어 앞으로 자주 사용하게 될 것\n",
    "- 이진 분류 문제에서 모델은 하나의 유닛과 sigmoid 활성화 함수를 가진 Dense층으로 끝나야 함. 이 모델의 출력은 확률을 나타내는 0과 1 사이의 스칼라 값\n",
    "- 이진 분류 문제에서 이런 스칼라 시그모이드 출력에 대해 사용할 손실 함수는 binary_crossentropy\n",
    "- rmsprop 옵티마이저는 문제에 상관없이 일바적으로 충분히 좋은 선택\n",
    "- 훈련 데이터에 대해 성능이 향상됨에 따라 신경망은 과대적합되기(overfitting) 시작하고 이전에 본적없는 데이터에서는 결과자 점점 나빠짐, 항상 훈련 세트 이외의 데이터에서 성능을 모니터링해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327b0847",
   "metadata": {},
   "source": [
    "## 4.2 뉴스 기사 분류: 다중 분류 문제 \n",
    "\n",
    "- 다중분류(multicalss classification): 클래스가 많음\n",
    "- 단일 레이블 다중 분류(single-label, multiclass classfication)문제: 각 데이터 포인트가 정확히 하나의 범주로 분류됨 <br>\n",
    "- 다중 레이블 다중 분류(multi-label, multiclass classification)문제: 각 데이터 포인트가 여러개의 점주에 속할 수 있다면, 이건 이런 문제 <br>\n",
    "\n",
    "### 4.2.1 로이터 데이터셋\n",
    "\n",
    "#### 코드 4-11 로이터 데이터셋 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a3f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "\n",
    "(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n",
    "\n",
    "print(len(train_data))\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15c310e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85da1238",
   "metadata": {},
   "source": [
    "#### 코드 4-12 로이터 데이터셋을 텍스트로 디코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8618fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = reuters.get_word_index()\n",
    "reverse_word_index= dict(\n",
    "    [(value, key) for (key, value) in word_index.items()])\n",
    "decoded_newswire = \" \".join(\n",
    "    [reverse_word_index.get(i-3, \"?\") for i in train_data[0]])  \n",
    "# 0, 1, 2는 '패딩', '문서 시작', '사전에 없음'을 위해 예약되어 있으므로 인덱스에서 3을 뺌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807573c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_labels[0])  # 샘플에 연결된 레이블은 토픽의 인덱스로 0과 45 사이의 정수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25778b9",
   "metadata": {},
   "source": [
    "### 4.2.2 데이터 준비\n",
    "\n",
    "이전의 예제와 동일한 코드를 사용해서 데이터를 벡터로 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255d3ba1",
   "metadata": {},
   "source": [
    "#### 코드 4-13 데이터 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446c7862",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)  # 훈련 데이터 벡터 변환\n",
    "x_test = vectorize_sequences(test_data)  # 훈련 데이터 벡터 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dab9029",
   "metadata": {},
   "source": [
    "레이블을 벡터로 바꾸는 방법은 2가지: <br>\n",
    "\n",
    "1. 레이블의 리스트를 정수 텐서로 변환하는 것 <br>\n",
    "2. one-hot encoding을 시용 <br>\n",
    "    - one-hot encoding이 범주형 데이터에 널리 사용되기 때문에 범주형 인코딩(categorical encoding)이라고도 부름 <br>\n",
    "    - 이 경우 레이블의 원-핫 인코딩은 각 레이블의 인덱스 자리는 1이고 나머지는 모두 0인 벡터, 밑에 코드는 예시 <br><br>\n",
    "    \n",
    "#### 코드 4-14 레이블 인코딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db050366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(labels, dimension=46):\n",
    "    results = np.zeros((len(labels), dimension))\n",
    "    for i, label in enumerate(labels):\n",
    "        results[i, label] =1.\n",
    "    return results\n",
    "y_train = to_one_hot(train_labels)   # 훈련 레이블 벡터 변환\n",
    "y_test = to_one_hot(test_labels)  # 테스트 레이블 벡터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b35b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = to_categorical(train_labels)\n",
    "y_test = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9a99a",
   "metadata": {},
   "source": [
    "### 4.2.3 모델 구성\n",
    "\n",
    "- 출력 클래스의 개수가 2에서 46개로 늘어남, 출력 공간의차원이 훨씬 커졌다. <br>\n",
    "- 각 층은 잠재적으로 정보의 병목(information bottleneck)이 될 수 있음 <br><br>\n",
    "\n",
    "#### 코드 4-15 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1596315",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(64, activation=\"relu\"),\n",
    "    layers.Dense(46, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f6eb95",
   "metadata": {},
   "source": [
    "주목해야할 점 2가지\n",
    "\n",
    "1) 마지막 Dense층의 크기가 46, 각 입력 샘플에 대해 46차원의 벡터를 출력한다는 뜻 <br>\n",
    "2) 마지막 층에 softmax 활성화 함수가 사용, 즉 46차원의 출력 벡터를 만들면 output[i]는 어떤 샘플이 클래스 i에 속할 확률 <br><br>\n",
    "=> 이런 문제에 사용활 최선의 손실 함수는 categorical_crossentropy, 이 함수는 두 확률 분포 사이의 거리를 측정 <br>\n",
    "여기에서는 모델이 출력한 확률 분포와 진짜 레이블의 분포 사이의 거리, 두 분포 사이의 거리를 최소화함으로써 진짜 레이블에 가능한 가까운 출력을 내도록 모델을 훈련하게 된다. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa17ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"rmsprop\",\n",
    "             loss=\"categorical_crossentropy\",\n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2fd8f3",
   "metadata": {},
   "source": [
    "#### 코드 4-17 검증 세트 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672e698",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[:10000]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a7f11",
   "metadata": {},
   "source": [
    "#### 코드 4-18 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dddd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(partial_x_train,\n",
    "         partial_y_train,\n",
    "         epochs=20,  # 20번 줌 \n",
    "         batch_size=512,\n",
    "         validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3955b773",
   "metadata": {},
   "source": [
    "#### 코드 4-19 훈련과 검증 손실 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11286a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(loss)+1)\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b28646",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64febbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a312678f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
