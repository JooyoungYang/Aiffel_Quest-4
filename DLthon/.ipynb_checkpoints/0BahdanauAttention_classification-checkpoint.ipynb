{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851be0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from konlpy.tag import Mecab\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0dbe6",
   "metadata": {},
   "source": [
    "### Step1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de652de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path =\"~/data/train.csv\"\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "train_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f788faf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0632cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\n",
    "    '협박 대화': 0,\n",
    "    '갈취 대화': 1,\n",
    "    '직장 내 괴롭힘 대화': 2,\n",
    "    '기타 괴롭힘 대화': 3\n",
    "    # 이와 같이 레이블과 숫자를 매핑해줍니다.\n",
    "}\n",
    "train_data['class'] = train_data['class'].replace(label_mapping).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2409604",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2693aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e297dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_path =\"~/data/test.json\"\n",
    "test_data = pd.read_json(test_data_path)\n",
    "test_data = test_data.transpose()\n",
    "test_data.to_csv(\"test_data.csv\", mode=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9042db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e41cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test_data.csv', index_col= 0)\n",
    "test_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dbd4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_answersheet_path =\"~/data/sample_answersheet.json\"\n",
    "# sample_answersheet_data = pd.read_json(sample_answersheet_path)\n",
    "# sample_answersheet_data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa46836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf93c66a",
   "metadata": {},
   "source": [
    "1. 데이터 살펴보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d42aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.groupby('class').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72db7fef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db77136",
   "metadata": {},
   "source": [
    "2. 데이터 중복 제거 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0343576",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8fc9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus_cov = list(set(train_data.conversation))\n",
    "print(\"Data Size:\", len(cleaned_corpus_cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc2743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cleaned_corpus_cov)[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0d1377",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cleaned_corpus_cov)[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632ea798",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(cleaned_corpus_cov)[51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6db019f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1058d98",
   "metadata": {},
   "source": [
    "3. 데이터 분포 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e42a499",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "cleaned_corpus_cov = list(set(train_data.conversation))  # set를 사용해서 중복을 제거합니다.\n",
    "print(\"Data Size:\", len(cleaned_corpus_cov))\n",
    "\n",
    "\n",
    "\n",
    "# 한글 폰트 설정\n",
    "if platform.system() == 'Windows':\n",
    "    plt.rc('font', family='Malgun Gothic')\n",
    "elif platform.system() == 'Darwin':\n",
    "    plt.rc('font', family='AppleGothic')\n",
    "else:\n",
    "    plt.rc('font', family='NanumGothic')  # 리눅스의 경우 나눔고딕 폰트를 사용하도록 변경해주세요.\n",
    "\n",
    "# 그래프에서 마이너스 기호가 표시되도록 설정\n",
    "plt.rc('axes', unicode_minus=False)\n",
    "\n",
    "\n",
    "\n",
    "for sen in cleaned_corpus_cov:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(cleaned_corpus_cov))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in cleaned_corpus_cov:   # 중복이 제거된 코퍼스 기준\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.xlabel(xlabel='글자 수')\n",
    "plt.ylabel(ylabel='빈도 수')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce123cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중복제거 \n",
    "ax = train_data.conversation.str.len().hist()\n",
    "\n",
    "max_length = train_data.conversation.str.len().max()\n",
    "min_length = train_data.conversation.str.len().min()\n",
    "mean_length = train_data.conversation.str.len().mean()\n",
    "\n",
    "ax.set_xlabel(xlabel='글자 수')\n",
    "ax.set_ylabel(ylabel='빈도 수')\n",
    "\n",
    "print(\"최대길이:\", max_length)\n",
    "print(\"최소길이:\", min_length)\n",
    "print(\"평균길이:\", mean_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2564f",
   "metadata": {},
   "source": [
    "중복을 제거하나 안하나 별 차이가 없다. 그냥 데이터 정제나 해야겠다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f618bc2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7269078a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007a60c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cleaned_corpus_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cbbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_400 = cleaned_corpus_cov[:400]\n",
    "cleaned_400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036c175d",
   "metadata": {},
   "source": [
    "### Step2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=True, e_token=True):\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", str(sentence))\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,가-힣ㄱ-ㅎㅏ-ㅣ0-9]+\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd1d473",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_data = list(map(preprocess_sentence, cleaned_corpus_cov))\n",
    "print(pre_data)\n",
    "print(len(pre_data))\n",
    "print(type(pre_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbf51ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_400 = list(map(preprocess_sentence, cleaned_400))\n",
    "print(pre_400)\n",
    "print(len(pre_400))\n",
    "print(type(pre_400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167911b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_test_data = list(map(preprocess_sentence, test_data.text))\n",
    "print(pre_test_data)\n",
    "print(len(pre_test_data))\n",
    "print(type(pre_test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283f4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # index_word로 바꾸기\n",
    "# index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "\n",
    "# for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "\n",
    "# index_to_word[index] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1b2ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize2(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    \n",
    "    mecab = Mecab()\n",
    "    corpus = [' '.join(mecab.morphs(sen)) for sen in corpus]\n",
    "        \n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='pre', maxlen=500)\n",
    "    \n",
    "    return tensor, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63167024",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_tensor, conv_tokenizer = tokenize2(pre_data)\n",
    "print(f'Converasation Vocab Size: {len(conv_tokenizer.index_word)}')\n",
    "conv400_tensor, conv400_tokenizer = tokenize2(pre_400)\n",
    "print(f'Converasation_400 Vocab Size: {len(conv_tokenizer.index_word)}')\n",
    "test_tensor, test_tokenizer = tokenize2(pre_test_data)\n",
    "print(f'Text Vocab Size: {len(test_tokenizer.index_word)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18895158",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(conv400_tensor, test_tensor, test_size=0.2)\n",
    "\n",
    "len(x_train), len(x_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b312448",
   "metadata": {},
   "source": [
    "### Step 3: 모델 설계\n",
    "\n",
    "1. BahdanauAttention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9635819",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e002d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units, return_sequences=True)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e2b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units, return_sequences=True, return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "        \n",
    "        self.dropout = tf.keras.layers.Dropout(rate=0.2)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "\n",
    "        out, h_dec = self.gru(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8ed400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드를 실행하세요.\n",
    "BATCH_SIZE     = 64\n",
    "SRC_VOCAB_SIZE = len(conv400_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(test_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 1024\n",
    "embedding_dim = 512\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a182214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b41ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b900061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2b3b0121",
   "metadata": {},
   "source": [
    "### Step 4: 훈련하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c6c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d9592",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']]*bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f435a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # tqdm\n",
    "import random\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, x_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)    # tqdm\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(x_train[idx:idx+BATCH_SIZE],\n",
    "                                y_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                test_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))    # tqdm\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))    # tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e03d8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0debd7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce567d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc389f67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcf2e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
