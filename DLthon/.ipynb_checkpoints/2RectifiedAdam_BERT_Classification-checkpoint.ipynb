{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "778a7d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e6709ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>class</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>협박 대화</td>\n",
       "      <td>길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>기타 괴롭힘 대화</td>\n",
       "      <td>너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>갈취 대화</td>\n",
       "      <td>저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx      class                                       conversation\n",
       "0    0      협박 대화  지금 너 스스로를 죽여달라고 애원하는 것인가?\\n 아닙니다. 죄송합니다.\\n 죽을 ...\n",
       "1    1      협박 대화  길동경찰서입니다.\\n9시 40분 마트에 폭발물을 설치할거다.\\n네?\\n똑바로 들어 ...\n",
       "2    2  기타 괴롭힘 대화  너 되게 귀여운거 알지? 나보다 작은 남자는 첨봤어.\\n그만해. 니들 놀리는거 재미...\n",
       "3    3      갈취 대화  어이 거기\\n예??\\n너 말이야 너. 이리 오라고\\n무슨 일.\\n너 옷 좋아보인다?...\n",
       "4    4      갈취 대화  저기요 혹시 날이 너무 뜨겁잖아요? 저희 회사에서 이 선크림 파는데 한 번 손등에 ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_path = \"~/data/train.csv\"\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06272d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>conversation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>갈취 대화</th>\n",
       "      <td>981</td>\n",
       "      <td>981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>기타 괴롭힘 대화</th>\n",
       "      <td>1094</td>\n",
       "      <td>1094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>직장 내 괴롭힘 대화</th>\n",
       "      <td>979</td>\n",
       "      <td>979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>협박 대화</th>\n",
       "      <td>896</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              idx  conversation\n",
       "class                          \n",
       "갈취 대화         981           981\n",
       "기타 괴롭힘 대화    1094          1094\n",
       "직장 내 괴롭힘 대화   979           979\n",
       "협박 대화         896           896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(by=['class']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e55e5",
   "metadata": {},
   "source": [
    "### Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2369e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encode = {\n",
    "    \"협박 대화\" : 0,\n",
    "    \"갈취 대화\" : 1,\n",
    "    \"직장 내 괴롭힘 대화\" : 2,\n",
    "    \"기타 괴롭힘 대화\" : 3,   \n",
    "}\n",
    "train_df['encoded_label'] = train_df['class'].map(label_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "117a111e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       0\n",
       "2       3\n",
       "3       1\n",
       "4       1\n",
       "       ..\n",
       "3945    3\n",
       "3946    1\n",
       "3947    2\n",
       "3948    1\n",
       "3949    2\n",
       "Name: encoded_label, Length: 3950, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['encoded_label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ae300aa",
   "metadata": {},
   "source": [
    "### Spliting data into training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3fa9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['conversation'].to_list()\n",
    "train_labels = train_df['encoded_label'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "345792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Stratified Split Train and Validation data \n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=0.2, random_state=1004, stratify=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080c441",
   "metadata": {},
   "source": [
    "# 2. Tokenizing the text\n",
    "## Load Tokenizer and Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac78679",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"klue/bert-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f5d56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "# Load Tokenizer \n",
    "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Tokenizing\n",
    "# dict_keys(['input_ids', 'token_type_ids', 'attention_mask']) 이런식으로 \n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True) \n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae205483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n"
     ]
    }
   ],
   "source": [
    "print(dict(val_encodings).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994945f",
   "metadata": {},
   "source": [
    "# 3. Creating a Dataset Object for Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "689bff16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# trainset-set\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels\n",
    "))\n",
    "\n",
    "# validation-set\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ef1b33",
   "metadata": {},
   "source": [
    "# 4. Fine-Tuning BERT\n",
    "## 4.1 Using Native Tensorflow pipeline\n",
    "### Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "231c76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForSequenceClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/conda/lib/python3.9/site-packages/keras/optimizer_v2/optimizer_v2.py:355: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertForSequenceClassification\n",
    "from tensorflow.keras import optimizers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "num_labels = len(label_encode)\n",
    "print(num_labels)\n",
    "# TODO : from_pt=False 혹은 없이 해보기\n",
    "# from_pt – (optional) boolean, default False: Load the model weights from a PyTorch state_dict save file (see docstring of pretrained_model_name_or_path argument).\n",
    "model = TFBertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=num_labels, from_pt=True)\n",
    "\n",
    "optimizer = tfa.optimizers.RectifiedAdam(lr=5.0e-5, total_steps = 2344*4, warmup_proportion=0.1, min_lr=1e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5a0eb084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method TFSequenceClassificationLoss.compute_loss of <transformers.models.bert.modeling_tf_bert.TFBertForSequenceClassification object at 0x7f8028efc790>>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49cd050",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "395/395 [==============================] - 447s 996ms/step - loss: 1.0944 - accuracy: 0.5266 - val_loss: 0.4741 - val_accuracy: 0.8557\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 2/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.3532 - accuracy: 0.8801 - val_loss: 0.4548 - val_accuracy: 0.8684\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 3/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.1978 - accuracy: 0.9383 - val_loss: 0.3958 - val_accuracy: 0.8911\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 4/10\n",
      "395/395 [==============================] - 392s 994ms/step - loss: 0.1103 - accuracy: 0.9646 - val_loss: 0.4749 - val_accuracy: 0.8924\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 5/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0599 - accuracy: 0.9842 - val_loss: 0.5986 - val_accuracy: 0.8646\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 6/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0227 - accuracy: 0.9943 - val_loss: 0.4862 - val_accuracy: 0.8949\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 7/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0119 - accuracy: 0.9972 - val_loss: 0.4960 - val_accuracy: 0.8937\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 8/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0076 - accuracy: 0.9987 - val_loss: 0.5032 - val_accuracy: 0.8962\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 9/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0061 - accuracy: 0.9987 - val_loss: 0.5138 - val_accuracy: 0.8962\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n",
      "Epoch 10/10\n",
      "395/395 [==============================] - 392s 993ms/step - loss: 0.0054 - accuracy: 0.9987 - val_loss: 0.5256 - val_accuracy: 0.8987\n",
      "WARNING:tensorflow:Can save best model only with vall_accuracy available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8028c46760>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint\n",
    "\n",
    "callback_earlystopping = EarlyStopping(\n",
    "    monitor=\"val_accuracy\", \n",
    "    min_delta=0.001, # the threshold that triggers the termination (acc should at least improve 0.001)\n",
    "    patience=2)\n",
    "\n",
    "callback_learningrate_scheduler = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    verbose=0,\n",
    "    mode='auto',\n",
    "    min_delta=0.0001,\n",
    "    cooldown=0,\n",
    "    min_lr=0,\n",
    ")\n",
    "\n",
    "\n",
    "callback_modelcheckpoint = ModelCheckpoint(\n",
    "    filepath = \"BERT_BestModel.keras\",\n",
    "    monitor=\"vall_accuracy\",\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "callback_list = [callback_earlystopping, callback_learningrate_scheduler, callback_modelcheckpoint]\n",
    "\n",
    "model.fit(\n",
    "    train_dataset.shuffle(1000).batch(8), epochs=10, batch_size=8,\n",
    "    validation_data=val_dataset.shuffle(1000).batch(16),\n",
    "    callbacks = callback_list\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf59540",
   "metadata": {},
   "source": [
    "### Saving the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "964e0c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_model/fine-tuned-klue-bert-base -- Folder already exists \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('_model/fine-tuned-klue-bert-base/tokenizer_config.json',\n",
       " '_model/fine-tuned-klue-bert-base/special_tokens_map.json',\n",
       " '_model/fine-tuned-klue-bert-base/vocab.txt',\n",
       " '_model/fine-tuned-klue-bert-base/added_tokens.json',\n",
       " '_model/fine-tuned-klue-bert-base/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'fine-tuned-klue-bert-base'\n",
    "MODEL_SAVE_PATH = os.path.join(\"_model\", MODEL_NAME) # change this to your preferred location\n",
    "\n",
    "if os.path.exists(MODEL_SAVE_PATH):\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder already exists \\n\")\n",
    "else:\n",
    "    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
    "    print(f\"{MODEL_SAVE_PATH} -- Folder create complete \\n\")\n",
    "\n",
    "# save tokenizer, model\n",
    "model.save_pretrained(MODEL_SAVE_PATH)\n",
    "tokenizer.save_pretrained(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee1563",
   "metadata": {},
   "source": [
    "# 6. Loading the saved Model and Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1dfba77b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at _model/fine-tuned-klue-bert-base were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at _model/fine-tuned-klue-bert-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "# Load Fine-tuning model\n",
    "loaded_tokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)\n",
    "loaded_model = TFBertForSequenceClassification.from_pretrained(MODEL_SAVE_PATH)\n",
    "\n",
    "text_classifier = TextClassificationPipeline(\n",
    "    tokenizer=loaded_tokenizer, \n",
    "    model=loaded_model, \n",
    "    framework='tf',\n",
    "    return_all_scores=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b5f9a3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [file_name, class]\n",
       "Index: []"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(columns=['file_name', 'class'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf826cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t_000</th>\n",
       "      <td>아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_001</th>\n",
       "      <td>우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_002</th>\n",
       "      <td>너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_004</th>\n",
       "      <td>아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_005</th>\n",
       "      <td>그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_495</th>\n",
       "      <td>미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_496</th>\n",
       "      <td>교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_497</th>\n",
       "      <td>야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_498</th>\n",
       "      <td>야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t_499</th>\n",
       "      <td>엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "t_000  아가씨 담배한갑주소 네 4500원입니다 어 네 지갑어디갔지 에이 버스에서 잃어버렸나...\n",
       "t_001  우리팀에서 다른팀으로 갈 사람 없나? 그럼 영지씨가 가는건 어때?  네? 제가요? ...\n",
       "t_002  너 오늘 그게 뭐야 네 제가 뭘 잘못했나요.? 제대로 좀 하지 네 똑바로 좀 하지 ...\n",
       "t_004  아무튼 앞으로 니가 내 와이파이야. .응 와이파이 온. 켰어. 반말? 주인님이라고도...\n",
       "t_005  그러니까 빨리 말해. 선생님 제발 살려주십시오.  비밀번호 틀릴 때마다 손톱 하나씩...\n",
       "...                                                  ...\n",
       "t_495  미나씨 휴가 결제 올리기 전에 저랑 상의하라고 말한거 기억해요? 네 합니다. 보고서...\n",
       "t_496  교수님 제 논문에 제 이름이 없나요?  아 무슨 논문말이야?  지난 번 냈던 논문이...\n",
       "t_497  야 너  네 저요? 그래 너 왜요 돈좀 줘봐  돈 없어요 돈이 왜 없어 지갑은 폼이...\n",
       "t_498  야 너 빨리 안 뛰어와? 너 이 환자 제대로 봤어 안 봤어 어제 저녁부터 계속 보다...\n",
       "t_499  엄마 저 그 돈 안해주시면 정말 큰일나요.  이유도 말하지 않고. 몇번째니 경민아....\n",
       "\n",
       "[400 rows x 1 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('./test_data.csv', index_col=0)\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "baeb12d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('/aiffel/data/test.json', 'r') as f:\n",
    "    test_json = json.load(f)\n",
    "    \n",
    "# test_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "635a9058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a2dab4908db4c50ac2acc82f89323fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/400 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'t_000': 1,\n",
       " 't_001': 2,\n",
       " 't_002': 2,\n",
       " 't_004': 3,\n",
       " 't_005': 0,\n",
       " 't_006': 0,\n",
       " 't_007': 3,\n",
       " 't_009': 1,\n",
       " 't_010': 0,\n",
       " 't_012': 2,\n",
       " 't_014': 2,\n",
       " 't_015': 0,\n",
       " 't_018': 0,\n",
       " 't_019': 3,\n",
       " 't_020': 0,\n",
       " 't_021': 3,\n",
       " 't_022': 3,\n",
       " 't_023': 1,\n",
       " 't_024': 1,\n",
       " 't_025': 2,\n",
       " 't_028': 2,\n",
       " 't_030': 1,\n",
       " 't_031': 0,\n",
       " 't_033': 1,\n",
       " 't_034': 3,\n",
       " 't_035': 3,\n",
       " 't_036': 3,\n",
       " 't_037': 0,\n",
       " 't_038': 0,\n",
       " 't_039': 1,\n",
       " 't_040': 0,\n",
       " 't_041': 1,\n",
       " 't_042': 2,\n",
       " 't_043': 1,\n",
       " 't_045': 2,\n",
       " 't_046': 0,\n",
       " 't_047': 3,\n",
       " 't_049': 0,\n",
       " 't_050': 1,\n",
       " 't_051': 3,\n",
       " 't_052': 1,\n",
       " 't_053': 2,\n",
       " 't_054': 2,\n",
       " 't_055': 3,\n",
       " 't_056': 3,\n",
       " 't_058': 3,\n",
       " 't_059': 3,\n",
       " 't_060': 1,\n",
       " 't_061': 3,\n",
       " 't_062': 0,\n",
       " 't_063': 3,\n",
       " 't_064': 2,\n",
       " 't_065': 0,\n",
       " 't_066': 3,\n",
       " 't_067': 3,\n",
       " 't_069': 1,\n",
       " 't_071': 2,\n",
       " 't_072': 3,\n",
       " 't_073': 2,\n",
       " 't_074': 0,\n",
       " 't_076': 2,\n",
       " 't_077': 1,\n",
       " 't_078': 2,\n",
       " 't_081': 0,\n",
       " 't_083': 0,\n",
       " 't_084': 3,\n",
       " 't_085': 2,\n",
       " 't_086': 0,\n",
       " 't_088': 1,\n",
       " 't_089': 2,\n",
       " 't_091': 2,\n",
       " 't_092': 0,\n",
       " 't_093': 3,\n",
       " 't_095': 2,\n",
       " 't_097': 2,\n",
       " 't_098': 1,\n",
       " 't_099': 0,\n",
       " 't_101': 1,\n",
       " 't_103': 0,\n",
       " 't_104': 0,\n",
       " 't_105': 2,\n",
       " 't_106': 2,\n",
       " 't_107': 3,\n",
       " 't_109': 0,\n",
       " 't_111': 0,\n",
       " 't_112': 0,\n",
       " 't_113': 2,\n",
       " 't_114': 1,\n",
       " 't_115': 1,\n",
       " 't_117': 2,\n",
       " 't_118': 2,\n",
       " 't_119': 1,\n",
       " 't_120': 2,\n",
       " 't_121': 3,\n",
       " 't_122': 1,\n",
       " 't_123': 1,\n",
       " 't_124': 1,\n",
       " 't_125': 1,\n",
       " 't_126': 2,\n",
       " 't_128': 1,\n",
       " 't_130': 3,\n",
       " 't_131': 3,\n",
       " 't_132': 2,\n",
       " 't_133': 3,\n",
       " 't_134': 1,\n",
       " 't_135': 0,\n",
       " 't_136': 3,\n",
       " 't_137': 0,\n",
       " 't_138': 3,\n",
       " 't_140': 0,\n",
       " 't_142': 1,\n",
       " 't_143': 3,\n",
       " 't_144': 2,\n",
       " 't_145': 0,\n",
       " 't_146': 0,\n",
       " 't_147': 0,\n",
       " 't_148': 0,\n",
       " 't_149': 1,\n",
       " 't_150': 0,\n",
       " 't_151': 3,\n",
       " 't_156': 0,\n",
       " 't_157': 2,\n",
       " 't_159': 3,\n",
       " 't_160': 0,\n",
       " 't_161': 3,\n",
       " 't_163': 0,\n",
       " 't_164': 2,\n",
       " 't_166': 2,\n",
       " 't_168': 3,\n",
       " 't_170': 0,\n",
       " 't_171': 2,\n",
       " 't_172': 2,\n",
       " 't_173': 3,\n",
       " 't_174': 2,\n",
       " 't_175': 3,\n",
       " 't_176': 0,\n",
       " 't_177': 2,\n",
       " 't_178': 3,\n",
       " 't_179': 1,\n",
       " 't_180': 2,\n",
       " 't_181': 3,\n",
       " 't_182': 2,\n",
       " 't_183': 2,\n",
       " 't_184': 0,\n",
       " 't_187': 0,\n",
       " 't_188': 1,\n",
       " 't_189': 0,\n",
       " 't_190': 0,\n",
       " 't_192': 1,\n",
       " 't_193': 3,\n",
       " 't_194': 1,\n",
       " 't_195': 3,\n",
       " 't_196': 0,\n",
       " 't_198': 0,\n",
       " 't_199': 3,\n",
       " 't_200': 2,\n",
       " 't_201': 0,\n",
       " 't_202': 3,\n",
       " 't_203': 2,\n",
       " 't_204': 0,\n",
       " 't_205': 3,\n",
       " 't_206': 2,\n",
       " 't_207': 1,\n",
       " 't_208': 2,\n",
       " 't_209': 2,\n",
       " 't_211': 1,\n",
       " 't_212': 3,\n",
       " 't_213': 2,\n",
       " 't_214': 1,\n",
       " 't_218': 0,\n",
       " 't_219': 0,\n",
       " 't_220': 1,\n",
       " 't_222': 0,\n",
       " 't_224': 0,\n",
       " 't_226': 1,\n",
       " 't_227': 0,\n",
       " 't_228': 1,\n",
       " 't_229': 0,\n",
       " 't_230': 2,\n",
       " 't_231': 0,\n",
       " 't_232': 3,\n",
       " 't_233': 3,\n",
       " 't_234': 1,\n",
       " 't_235': 3,\n",
       " 't_236': 3,\n",
       " 't_237': 1,\n",
       " 't_238': 0,\n",
       " 't_239': 3,\n",
       " 't_240': 3,\n",
       " 't_241': 3,\n",
       " 't_242': 3,\n",
       " 't_243': 2,\n",
       " 't_245': 1,\n",
       " 't_246': 2,\n",
       " 't_247': 1,\n",
       " 't_248': 2,\n",
       " 't_249': 0,\n",
       " 't_250': 0,\n",
       " 't_252': 0,\n",
       " 't_253': 2,\n",
       " 't_254': 2,\n",
       " 't_255': 1,\n",
       " 't_257': 2,\n",
       " 't_258': 1,\n",
       " 't_259': 2,\n",
       " 't_260': 1,\n",
       " 't_262': 0,\n",
       " 't_263': 3,\n",
       " 't_266': 3,\n",
       " 't_269': 1,\n",
       " 't_270': 0,\n",
       " 't_272': 0,\n",
       " 't_273': 2,\n",
       " 't_275': 2,\n",
       " 't_276': 2,\n",
       " 't_277': 3,\n",
       " 't_278': 2,\n",
       " 't_279': 2,\n",
       " 't_280': 1,\n",
       " 't_281': 0,\n",
       " 't_282': 3,\n",
       " 't_283': 2,\n",
       " 't_285': 2,\n",
       " 't_286': 1,\n",
       " 't_287': 2,\n",
       " 't_288': 1,\n",
       " 't_289': 3,\n",
       " 't_290': 1,\n",
       " 't_291': 1,\n",
       " 't_292': 3,\n",
       " 't_293': 3,\n",
       " 't_294': 2,\n",
       " 't_295': 3,\n",
       " 't_298': 1,\n",
       " 't_299': 0,\n",
       " 't_301': 3,\n",
       " 't_302': 0,\n",
       " 't_303': 2,\n",
       " 't_304': 1,\n",
       " 't_305': 1,\n",
       " 't_306': 3,\n",
       " 't_307': 2,\n",
       " 't_308': 1,\n",
       " 't_309': 2,\n",
       " 't_310': 0,\n",
       " 't_311': 1,\n",
       " 't_312': 3,\n",
       " 't_314': 2,\n",
       " 't_315': 3,\n",
       " 't_316': 1,\n",
       " 't_317': 3,\n",
       " 't_318': 3,\n",
       " 't_319': 0,\n",
       " 't_320': 2,\n",
       " 't_321': 0,\n",
       " 't_322': 0,\n",
       " 't_323': 1,\n",
       " 't_324': 1,\n",
       " 't_325': 1,\n",
       " 't_326': 3,\n",
       " 't_328': 2,\n",
       " 't_329': 3,\n",
       " 't_330': 3,\n",
       " 't_331': 0,\n",
       " 't_332': 0,\n",
       " 't_333': 3,\n",
       " 't_334': 1,\n",
       " 't_336': 3,\n",
       " 't_337': 1,\n",
       " 't_339': 3,\n",
       " 't_340': 0,\n",
       " 't_341': 2,\n",
       " 't_342': 0,\n",
       " 't_343': 0,\n",
       " 't_344': 1,\n",
       " 't_345': 2,\n",
       " 't_347': 1,\n",
       " 't_348': 0,\n",
       " 't_349': 0,\n",
       " 't_350': 0,\n",
       " 't_351': 3,\n",
       " 't_352': 1,\n",
       " 't_354': 1,\n",
       " 't_355': 2,\n",
       " 't_356': 3,\n",
       " 't_357': 3,\n",
       " 't_358': 0,\n",
       " 't_360': 3,\n",
       " 't_361': 0,\n",
       " 't_362': 1,\n",
       " 't_363': 1,\n",
       " 't_364': 2,\n",
       " 't_366': 3,\n",
       " 't_367': 2,\n",
       " 't_368': 0,\n",
       " 't_369': 1,\n",
       " 't_370': 2,\n",
       " 't_371': 2,\n",
       " 't_372': 0,\n",
       " 't_373': 2,\n",
       " 't_374': 2,\n",
       " 't_375': 1,\n",
       " 't_376': 1,\n",
       " 't_377': 3,\n",
       " 't_378': 1,\n",
       " 't_379': 1,\n",
       " 't_380': 2,\n",
       " 't_381': 0,\n",
       " 't_382': 2,\n",
       " 't_383': 2,\n",
       " 't_384': 3,\n",
       " 't_385': 1,\n",
       " 't_387': 0,\n",
       " 't_388': 0,\n",
       " 't_389': 3,\n",
       " 't_390': 1,\n",
       " 't_391': 0,\n",
       " 't_392': 1,\n",
       " 't_393': 2,\n",
       " 't_394': 1,\n",
       " 't_395': 2,\n",
       " 't_396': 0,\n",
       " 't_398': 1,\n",
       " 't_399': 0,\n",
       " 't_400': 3,\n",
       " 't_401': 3,\n",
       " 't_402': 2,\n",
       " 't_403': 2,\n",
       " 't_405': 3,\n",
       " 't_406': 2,\n",
       " 't_408': 2,\n",
       " 't_409': 1,\n",
       " 't_410': 3,\n",
       " 't_411': 3,\n",
       " 't_412': 0,\n",
       " 't_413': 2,\n",
       " 't_416': 3,\n",
       " 't_418': 1,\n",
       " 't_419': 0,\n",
       " 't_420': 1,\n",
       " 't_421': 0,\n",
       " 't_422': 3,\n",
       " 't_423': 3,\n",
       " 't_424': 3,\n",
       " 't_425': 1,\n",
       " 't_426': 1,\n",
       " 't_428': 2,\n",
       " 't_429': 0,\n",
       " 't_430': 3,\n",
       " 't_432': 1,\n",
       " 't_433': 0,\n",
       " 't_436': 0,\n",
       " 't_439': 2,\n",
       " 't_440': 3,\n",
       " 't_441': 0,\n",
       " 't_442': 2,\n",
       " 't_443': 3,\n",
       " 't_445': 2,\n",
       " 't_446': 0,\n",
       " 't_447': 2,\n",
       " 't_448': 2,\n",
       " 't_450': 3,\n",
       " 't_451': 0,\n",
       " 't_452': 2,\n",
       " 't_453': 3,\n",
       " 't_455': 1,\n",
       " 't_456': 2,\n",
       " 't_457': 2,\n",
       " 't_458': 1,\n",
       " 't_459': 1,\n",
       " 't_461': 1,\n",
       " 't_462': 0,\n",
       " 't_463': 2,\n",
       " 't_464': 2,\n",
       " 't_465': 3,\n",
       " 't_466': 3,\n",
       " 't_467': 2,\n",
       " 't_469': 0,\n",
       " 't_470': 2,\n",
       " 't_471': 1,\n",
       " 't_472': 0,\n",
       " 't_474': 3,\n",
       " 't_475': 2,\n",
       " 't_476': 1,\n",
       " 't_479': 2,\n",
       " 't_481': 3,\n",
       " 't_482': 1,\n",
       " 't_483': 0,\n",
       " 't_484': 1,\n",
       " 't_485': 3,\n",
       " 't_489': 3,\n",
       " 't_490': 3,\n",
       " 't_492': 0,\n",
       " 't_493': 1,\n",
       " 't_494': 1,\n",
       " 't_495': 2,\n",
       " 't_496': 2,\n",
       " 't_497': 1,\n",
       " 't_498': 2,\n",
       " 't_499': 0}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "answer_dict = {}\n",
    "for file_name, text in tqdm(test_json.items()):\n",
    "    preds_list = text_classifier(text['text'])[0]\n",
    "    best_label = int(sorted(preds_list, key=lambda x : x['score'])[-1]['label'].split('_')[-1])\n",
    "    answer_dict[file_name] = best_label\n",
    "          \n",
    "answer_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66d02902",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in answer_dict.items():\n",
    "    test_df = test_df.append({'file_name': key, 'class': value}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2f5ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_df.to_csv(\"HanBERT.csv\")  # accuracy - 0.9125\n",
    "# test_df.to_csv(\"HanBERT_ver2.csv\")  # accuracy - 0.875\n",
    "test_df.to_csv(\"HanBERT_ver3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e66b4aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc33a51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbc456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319c1473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3465c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e2fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
