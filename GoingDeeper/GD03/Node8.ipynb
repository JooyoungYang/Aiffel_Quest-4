{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4846361b",
   "metadata": {},
   "source": [
    "8-1 들어가며 <br>\n",
    "8-2 워드 임베딩의 편향성 <br>\n",
    "8-3 WEAT를 통한 편향성 측정 <br>\n",
    "8-4 WEAT 구현하기 <br>\n",
    "8-5 사전 학습된 Word Embeeding에 WEAT 적용<br>\n",
    "8-6 직접 만드는 Word Embedding에 WEAT 적용(1) <br>\n",
    "8-7 직접 만드는 Word Embedding에 WEAT 적용(2) <br>\n",
    "\n",
    "### 8-2 워드 임베딩의 편향성\n",
    "<b> Word Embedding Association Test(WEAT)</b>는 임베딩 모델의 편향을 측정하는 방식 중 하나 <br>\n",
    "\n",
    "### 8-3 WEAT 구현하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bec773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41fbd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = {\n",
    "    '장미': [4.1, 1.2, -2.4, 0.5, 4.1],\n",
    "    '튤립': [3.1, 0.5, 3.6, 1.7, 5.8],\n",
    "    '백합': [2.9, -1.3, 0.4, 1.1, 3.7],\n",
    "    '데이지': [5.4, 2.5, 4.6, -1.0, 3.6]\n",
    "}\n",
    "target_Y = {\n",
    "    '거미': [-1.5, 0.2, -0.6, -4.6, -5.3],\n",
    "    '모기': [0.4, 0.7, -1.9, -4.5, -2.9],\n",
    "    '파리': [0.9, 1.4, -2.3, -3.9, -4.7],\n",
    "    '메뚜기': [0.7, 0.9, -0.4, -4.1, -3.9]\n",
    "}\n",
    "attribute_A = {\n",
    "    '사랑':[2.8,  4.2, 4.3,  0.3, 5.0],\n",
    "    '행복':[3.8,  3. , -1.2,  4.4, 4.9],\n",
    "    '웃음':[3.7, -0.3,  1.2, -2.5, 3.9]\n",
    "}\n",
    "attribute_B = {\n",
    "    '재난': [-0.2, -2.8, -4.7, -4.3, -4.7],\n",
    "    '고통': [-4.5, -2.1,  -3.8, -3.6, -3.1],\n",
    "    '증오': [-3.6, -3.3, -3.5,  -3.7, -4.4]\n",
    "}\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae083cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([v for v in target_X.values()])\n",
    "Y = np.array([v for v in target_Y.values()])\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e572f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([v for v in attribute_A.values()])\n",
    "B = np.array([v for v in attribute_B.values()])\n",
    "print(A)\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994a5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(i, j):\n",
    "    return dot(i, j.T)/(norm(i)*norm(j))\n",
    "\n",
    "def s(w, A, B):\n",
    "    c_a = cos_sim(w, A)\n",
    "    c_b = cos_sim(w, B)\n",
    "    mean_A = np.mean(c_a, axis=-1)\n",
    "    mean_B = np.mean(c_b, axis=-1)\n",
    "    return mean_A - mean_B #, c_a, c_b\n",
    "\n",
    "print(s(target_X['장미'], A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08b341",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s(target_Y['거미'], A, B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c64335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s(X, A, B))\n",
    "print(round(np.mean(s(X, A, B)), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s(Y, A, B))\n",
    "print(round(np.mean(s(Y, A, B)), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f691e1",
   "metadata": {},
   "source": [
    "mean x∈ Xs(x,A,B)−mean y∈Ys(y,A,B)stdw∈X∪Ys(w,A,B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd14328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weat_score(X, Y, A, B):\n",
    "    \n",
    "    s_X = s(X, A, B)\n",
    "    s_Y = s(Y, A, B)\n",
    "\n",
    "    mean_X = np.mean(s_X)\n",
    "    mean_Y = np.mean(s_Y)\n",
    "    \n",
    "    std_dev = np.std(np.concatenate([s_X, s_Y], axis=0))\n",
    "    \n",
    "    return  (mean_X-mean_Y)/std_dev\n",
    "\n",
    "print(round(weat_score(X, Y, A, B), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9aeae8",
   "metadata": {},
   "source": [
    "### 8-5 사전 학습된 Word Embedding에 WEAT 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01cec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = '~/aiffel/weat' \n",
    "model_dir = os.path.join(data_dir, 'GoogleNews-vectors-negative300.bin')\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 50만개의 단어만 활용합니다. 메모리가 충분하다면 limit 파라미터값을 생략하여 300만개를 모두 활용할 수 있습니다. \n",
    "w2v = KeyedVectors.load_word2vec_format(model_dir, binary=True, limit=500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bd7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350932f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(w2v.vocab))   # Gensim 3.X 버전까지는 w2v.vocab을 직접 접근할 수 있습니다. \n",
    "print(len(w2v.index_to_key))   # Gensim 4.0부터는 index_to_key를 활용해 vocab size를 알 수 있습니다. \n",
    "print(len(w2v['I']))                    # 혹은 단어를 key로 직접 vector를 얻을 수 있습니다. \n",
    "print(w2v.vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369e2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v['happy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a664254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['happy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583d2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6654cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.most_similar(positive=['school'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea939f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = ['science', 'technology', 'physics', 'chemistry', 'Einstein', 'NASA', 'experiment', 'astronomy']\n",
    "target_Y = ['poetry', 'art', 'Shakespeare', 'dance', 'literature', 'novel', 'symphony', 'drama']\n",
    "attribute_A = ['brother', 'father', 'uncle', 'grandfather', 'son', 'he', 'his', 'him']\n",
    "attribute_B = ['sister', 'mother', 'aunt', 'grandmother', 'daughter', 'she', 'hers', 'her']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba334b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['junk', 'canned', 'convenience', 'frozen', 'fast']\n",
    "attribute_B = ['health', 'beneficial', 'good', 'nourishing', 'nutritious']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594490ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = ['pizza', 'coke', 'hamburger', 'ham', 'ramen', 'icecream', 'candy']\n",
    "target_Y = ['salad', 'fruit', 'vegetable', 'herb', 'root', 'greens', 'wholesome']\n",
    "attribute_A = ['book', 'essay', 'dictionary', 'magazine', 'novel']\n",
    "attribute_B = ['news', 'report', 'statement', 'broadcast', 'word']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a1f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_X = ['diet', 'meal', 'salad', 'protein']\n",
    "target_Y = ['quantum', 'linguistics', 'neuroscience', 'brain']\n",
    "attribute_A = ['beauty', 'healthy', 'nutritious', 'good']\n",
    "attribute_B = ['research', 'study', 'institution', 'foundation']\n",
    "\n",
    "X = np.array([w2v[word] for word in target_X])\n",
    "Y = np.array([w2v[word] for word in target_Y])\n",
    "A = np.array([w2v[word] for word in attribute_A])\n",
    "B = np.array([w2v[word] for word in attribute_B])\n",
    "\n",
    "weat_score(X, Y, A, B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96793421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#메모리를 다시 비워줍시다.\n",
    "del w2v\n",
    "print(\"삭제 완료\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf8069f",
   "metadata": {},
   "source": [
    "### 8-6 직접 만드는 Word Embedding에 WEAT 적용(1)\n",
    "\n",
    "1. 형태소 분석기를 이용하여 품사가 명사인 경우, 해당 단어를 추출하기 <br>\n",
    "2. 추출된 결과로 embedding model 만들기 <br>\n",
    "3. TF/IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기 <br>\n",
    "4. embedding model과 단어 셋으로 WEAT score 구해보기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e88e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    for i in range(20):\n",
    "        print(file.readline(), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1027a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916577df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 15분정도 걸립니다.\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "tokenized = []\n",
    "with open(os.getenv('HOME')+'/aiffel/weat/synopsis.txt', 'r') as file:\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line: break\n",
    "        words = okt.pos(line, stem=True, norm=True)\n",
    "        res = []\n",
    "        for w in words:\n",
    "            if w[1] in [\"Noun\"]:      # \"Adjective\", \"Verb\" 등을 포함할 수도 있습니다.\n",
    "                res.append(w[0])    # 명사일 때만 tokenized 에 저장하게 됩니다. \n",
    "        tokenized.append(res)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2519c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tokenized))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb6df2",
   "metadata": {},
   "source": [
    "2. 추출된 결과로 embedding model 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d35d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# tokenized에 담긴 데이터를 가지고 나만의 Word2Vec을 생성합니다. (Gensim 4.0 기준)\n",
    "model = Word2Vec(tokenized, vector_size=100, window=5, min_count=3, sg=0)  \n",
    "model.wv.most_similar(positive=['영화'])\n",
    "\n",
    "# Gensim 3.X 에서는 아래와 같이 생성합니다. \n",
    "# model = Word2Vec(tokenized, size=100, window=5, min_count=3, sg=0)  \n",
    "# model.most_similar(positive=['영화'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad6972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['사랑'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91524676",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(positive=['연극'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fae31b",
   "metadata": {},
   "source": [
    "3. TF-IDF로 해당 데이터를 가장 잘 표현하는 단어 셋 만들기\n",
    "\n",
    "\n",
    "targets_X, targets_Y, attribute_A, attribute_B를 만들어주었던 것처럼 WEAT score를 구할 때 단어 셋을 만들어줘야 한다. 이제 두 축을 어떤 기준으로 잡고, 해당 축의 어떤 항목을 사용할지 정해야 한다.\n",
    "\n",
    "- 영화 구분 <br>\n",
    "    - synopsis_art.txt : 예술영화 <br>\n",
    "    - synopsis_gen.txt : 일반영화(상업영화) <br>\n",
    "    - 그 외는 독립영화 등으로 분류됩니다. <br>\n",
    "    \n",
    "- 장르 구분 <br>\n",
    "    - synopsis_SF.txt: SF <br>\n",
    "    - synopsis_가족.txt: 가족 <br>\n",
    "    - synopsis_공연.txt: 공연 <br>\n",
    "    - synopsis_공포(호러).txt: 공포(호러) <br>\n",
    "    - synopsis_기타.txt: 기타 <br>\n",
    "    - synopsis_다큐멘터리.txt: 다큐멘터리 <br>\n",
    "    - synopsis_드라마.txt: 드라마 <br>\n",
    "    - synopsis_멜로로맨스.txt: 멜로로맨스 <br>\n",
    "    - synopsis_뮤지컬.txt: 뮤지컬 <br>\n",
    "    - synopsis_미스터리.txt: 미스터리 <br>\n",
    "    - synopsis_범죄.txt: 범죄 <br>\n",
    "    - synopsis_사극.txt: 사극 <br>\n",
    "    - synopsis_서부극(웨스턴).txt: 서부극(웨스턴) <br>\n",
    "    - synopsis_성인물(에로).txt: 성인물(에로) <br>\n",
    "    - synopsis_스릴러.txt: 스릴러 <br>\n",
    "    - synopsis_애니메이션.txt: 애니메이션 <br>\n",
    "    - synopsis_액션.txt: 액션 <br>\n",
    "    - synopsis_어드벤처.txt: 어드벤처 <br>\n",
    "    - synopsis_전쟁.txt: 전쟁 <br>\n",
    "    - synopsis_코미디.txt: 코미디 <br>\n",
    "    - synopsis_판타지.txt: 판타지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc407d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "art_txt = 'synopsis_art.txt'\n",
    "gen_txt = 'synopsis_gen.txt'\n",
    "\n",
    "def read_token(file_name):\n",
    "    okt = Okt()\n",
    "    result = []\n",
    "    with open(os.getenv('HOME')+'/aiffel/weat/'+file_name, 'r') as fread: \n",
    "        print(file_name, '파일을 읽고 있습니다.')\n",
    "        while True:\n",
    "            line = fread.readline() \n",
    "            if not line: break \n",
    "            tokenlist = okt.pos(line, stem=True, norm=True) \n",
    "            for word in tokenlist:\n",
    "                if word[1] in [\"Noun\"]:#, \"Adjective\", \"Verb\"]:\n",
    "                    result.append((word[0])) \n",
    "    return ' '.join(result)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bab00c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform([art, gen])\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd100a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.vocabulary_['영화'])\n",
    "print(vectorizer.get_feature_names()[23976])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d69e5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0c1805",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = X[0].tocoo()   # art를 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "m2 = X[1].tocoo()   # gen을 TF-IDF로 표현한 sparse matrix를 가져옵니다. \n",
    "\n",
    "w1 = [[i, j] for i, j in zip(m1.col, m1.data)]\n",
    "w2 = [[i, j] for i, j in zip(m2.col, m2.data)]\n",
    "\n",
    "w1.sort(key=lambda x: x[1], reverse=True)   #art를 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. ㄺㄷㅁㅇ\n",
    "w2.sort(key=lambda x: x[1], reverse=True)   #gen을 구성하는 단어들을 TF-IDF가 높은 순으로 정렬합니다. \n",
    "\n",
    "print('예술영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w1[i][0]], end=', ')\n",
    "\n",
    "print('\\n')\n",
    "    \n",
    "print('일반영화를 대표하는 단어들:')\n",
    "for i in range(100):\n",
    "    print(vectorizer.get_feature_names()[w2[i][0]], end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cf00e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 15\n",
    "w1_, w2_ = [], []\n",
    "for i in range(100):\n",
    "    w1_.append(vectorizer.get_feature_names()[w1[i][0]])\n",
    "    w2_.append(vectorizer.get_feature_names()[w2[i][0]])\n",
    "\n",
    "# w1에만 있고 w2에는 없는, 예술영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "target_art, target_gen = [], []\n",
    "for i in range(100):\n",
    "    if (w1_[i] not in w2_) and (w1_[i] in model.wv): target_art.append(w1_[i])\n",
    "    if len(target_art) == n: break \n",
    "\n",
    "# w2에만 있고 w1에는 없는, 일반영화를 잘 대표하는 단어를 15개 추출한다.\n",
    "for i in range(100):\n",
    "    if (w2_[i] not in w1_) and (w2_[i] in model.wv): target_gen.append(w2_[i])\n",
    "    if len(target_gen) == n: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfa0671",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898442ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac1e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_txt = ['synopsis_drama.txt', 'synopsis_romance.txt', 'synopsis_action.txt', 'synopsis_comedy.txt', 'synopsis_war.txt', 'synopsis_horror.txt']\n",
    "genre_name = ['드라마', '멜로로맨스', '액션', '코미디', '전쟁', '공포(호러)']\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e553c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 약 10분정도 걸립니다.\n",
    "genre = []\n",
    "for file_name in genre_txt:\n",
    "    genre.append(read_token(file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec8095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(genre)\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a5463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = [X[i].tocoo() for i in range(X.shape[0])]\n",
    "\n",
    "w = [[[i, j] for i, j in zip(mm.col, mm.data)] for mm in m]\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i].sort(key=lambda x: x[1], reverse=True)\n",
    "attributes = []\n",
    "for i in range(len(w)):\n",
    "    print(genre_name[i], end=': ')\n",
    "    attr = []\n",
    "    j = 0\n",
    "    while (len(attr) < 15):\n",
    "        if vectorizer.get_feature_names()[w[i][j][0]] in model.wv:\n",
    "            attr.append(vectorizer.get_feature_names()[w[i][j][0]])\n",
    "            print(vectorizer.get_feature_names()[w[i][j][0]], end=', ')\n",
    "        j += 1\n",
    "    attributes.append(attr)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5998219c",
   "metadata": {},
   "source": [
    "4. embedding model과 단어 셋으로 WEAT score 구해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca874d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = [[0 for _ in range(len(genre_name))] for _ in range(len(genre_name))]\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7106a2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([model.wv[word] for word in target_art])\n",
    "Y = np.array([model.wv[word] for word in target_gen])\n",
    "\n",
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        A = np.array([model.wv[word] for word in attributes[i]])\n",
    "        B = np.array([model.wv[word] for word in attributes[j]])\n",
    "        matrix[i][j] = weat_score(X, Y, A, B)\n",
    "\n",
    "print(\"슝~\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bd7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(genre_name)-1):\n",
    "    for j in range(i+1, len(genre_name)):\n",
    "        print(genre_name[i], genre_name[j],matrix[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2b4eaf",
   "metadata": {},
   "source": [
    "WEAT score가 0.8 이상, -0.8 이하의 경우만 해석해 보면 아래와 같습니다. <br>\n",
    "\n",
    "예술영화와 일반영화, 그리고 드라마와 멜로로맨스의 WEAT score의 의미를 해석해보면 예술영화는 멜로로맨스, 일반영화는 드라마와 가깝다고 볼 수 있습니다. 부호가 마이너스이므로 사람의 편향과 반대라는 것을 알 수 있습니다. <br>\n",
    "예술영화와 일반영화, 그리고 멜로로맨스와 코미디의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 코디미는 일반 영화와 가깝다고 볼 수 있습니다. <br>\n",
    "예술영화와 일반영화, 그리고 멜로로맨스와 전쟁의 WEAT score의 의미를 해석해보면 예술 영화는 멜로로맨스와 가깝고, 전쟁은 일반 영화와 가깝다고 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074a73c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np; \n",
    "import seaborn as sns; \n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# 한글 지원 폰트\n",
    "sns.set(font='NanumGothic')\n",
    "\n",
    "# 마이너스 부호 \n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "ax = sns.heatmap(matrix, xticklabels=genre_name, yticklabels=genre_name, annot=True,  cmap='RdYlGn_r')\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943d9ade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077af81c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f11ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89347048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35898039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
